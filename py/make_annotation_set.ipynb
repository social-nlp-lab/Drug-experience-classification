{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:  \n",
    "\n",
    "annotations originally started with zooniverse platform. however, because of data quality issues from numerous annotators, we resorted to using only 3 annotators that were better trained and thus more consistent.  \n",
    "there are three sets. one used for training the annotators, one used to assess agreement between annotators, and the third is the final set that annotators were able to annotate on their own assuming a certain level of agreement was reached.  \n",
    "the training set consists of 30 randomly selected posts  \n",
    "the agreement set consists of 100 randomly selected posts. this set also happens to be the same set that is being annotated by domain experts.  \n",
    "the final sets consist of 125 posts each. this would give us approximately 500 total posts.   \n",
    "\n",
    "\n",
    "because of the issues with the platform post ids. i need to check to see if there is any overlap in the training and agreement sets using the text itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# data were randomly selected from posts that were already annotated with zooniverse but had disagreement initially.  \n",
    "training_dat = pd.read_csv(\"../data/annotation_sets/annotation_training.csv\") \n",
    "# rename aggreed to subject\n",
    "training_dat = training_dat.rename(columns={\"aggreed\":\"subject\"})\n",
    "# keep only columns we need: 5:20\n",
    "training_dat = training_dat[training_dat.columns[4:20]]\n",
    "expert_dat = pd.read_csv(\"../data/annotation_sets/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "subject_dat = pickle.load(open(\"../data/zooni/subject_dat.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dat['overlap'] = ''\n",
    "for post in expert_dat['text']:\n",
    "    if post in training_dat['text'].values:\n",
    "        expert_dat['overlap'][expert_dat['text'] == post] = 'yes'\n",
    "    else:\n",
    "        expert_dat['overlap'][expert_dat['text'] == post] = 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no     85\n",
      "yes    15\n",
      "Name: overlap, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(expert_dat['overlap'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign ids\n",
    "expert_dat['id'] = pd.merge(expert_dat, subject_dat, on='text')['subject_id']\n",
    "training_dat['id'] = pd.merge(training_dat, subject_dat, on='text')['subject_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cols\n",
    "expert_dat['connection'] = ''\n",
    "expert_dat['subject'] = ''\n",
    "objectives = training_dat.columns[3:16]\n",
    "# add cols\n",
    "for objective in objectives:\n",
    "    expert_dat[objective] = ''\n",
    "# export expert dat\n",
    "expert_dat.to_csv(\"../data/expert_dat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Datasets for each annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_json(\"../data/raw_dat/all_drug_posts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random 500 posts - not [removed] and not empty\n",
    "seed = 2024\n",
    "posts_sample = posts[(posts['text'] != '[removed]') & (posts['text'] != '')].sample(400, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocaine            0.355252\n",
      "opiates            0.273208\n",
      "benzodiazepines    0.241414\n",
      "stims              0.130126\n",
      "Name: subreddit, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(posts['subreddit'].value_counts()/len(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opiates            0.320\n",
      "cocaine            0.285\n",
      "benzodiazepines    0.250\n",
      "stims              0.145\n",
      "Name: subreddit, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get proportion of each subreddit in sample\n",
    "print(posts_sample['subreddit'].value_counts()/len(posts_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_sample['all_text'] = posts_sample['title'] + ' ' + posts_sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no    100\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# are any of the test set in the sample?\n",
    "overlap = []\n",
    "for post in expert_dat['text']:\n",
    "    if post in posts_sample['all_text'].values:\n",
    "        overlap.append('yes')\n",
    "    else:\n",
    "        overlap.append('no')\n",
    "\n",
    "print(pd.Series(overlap).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export sample\n",
    "posts_sample[['subreddit', 'all_text']].to_csv(\"../data/annotation_sets/posts_training.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training set\n",
    "import pickle\n",
    "pickle.dump(posts_sample, open(\"../data/posts_training.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
