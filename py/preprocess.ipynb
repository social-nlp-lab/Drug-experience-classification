{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laylabouzoubaa/opt/anaconda3/lib/python3.9/site-packages/openpyxl/worksheet/_read_only.py:79: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "# data\n",
    "# connection = pickle.load(open('../data/zooni/connection.pkl', 'rb'))\n",
    "# subject = pickle.load(open('../data/zooni/subject.pkl', 'rb'))\n",
    "# objective = pickle.load(open('../data/zooni/objective.pkl', 'rb'))\n",
    "# training\n",
    "# training = pd.read_csv('../data/annotation_sets/training.csv')\n",
    "# for now\n",
    "training = pickle.load(open('../data/zooni/annotations_0104_complete.pkl', 'rb'))\n",
    "training = training.drop_duplicates(subset=['text'])\n",
    "testing = pd.read_excel('../data/annotation_sets/expert_dat_LB2.xlsx')[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "1. lower\n",
    "2. remove html, punctuation\n",
    "3. fix contractions\n",
    "3. tokenize\n",
    "4. remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# normalize text\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    # remove html\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # fix unicode\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    \n",
    "    # expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection['text'] = connection['text'].apply(normalize_text)\n",
    "# subject['text'] = subject['text'].apply(normalize_text)\n",
    "# objective['text'] = objective['text'].apply(normalize_text)\n",
    "training['text'] = training['text'].apply(normalize_text)\n",
    "testing['text'] = testing['text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused cols\n",
    "training = training.drop(columns=['user_name','subject_ids', 'file', 'annotation_num', 'combo_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "# connection_traininig = training[['all_text', 'connection']]\n",
    "# subject_traininig = training[['all_text', 'subject']]\n",
    "# # objectives\n",
    "# objectives = ['Quality', 'Legality', 'Effects', 'Methods of Ingestion', 'Combination of Substances', 'Mental Health',\n",
    "#           'N/A', 'Other', 'Overdose', 'Nurturant Support & Morality', 'Withdrawal', 'Safety', 'Relapse']\n",
    "# # convert to lower\n",
    "# objectives = [x.lower() for x in objectives]\n",
    "# objective_traininig = training[['all_text'] + objectives] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split testing into test and validation\n",
    "np.random.seed(2024)\n",
    "msk = np.random.rand(len(testing)) < 0.5\n",
    "testing_validation = testing[msk]\n",
    "testing_test = testing[~msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "# pickle.dump(connection, open('../data/connection_clean.pkl', 'wb'))\n",
    "# pickle.dump(subject, open('../data/subject_clean.pkl', 'wb'))\n",
    "# pickle.dump(objective, open('../data/objective_clean.pkl', 'wb'))\n",
    "pickle.dump(training, open('../data/training_clean.pkl', 'wb'))\n",
    "pickle.dump(testing_validation, open('../data/validation_clean.pkl', 'wb'))\n",
    "pickle.dump(testing_test, open('../data/testing_clean.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
