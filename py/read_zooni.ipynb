{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data that have annotations from zooniverse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dat = \"../data/zooni/SUD Project Classifications_0105.csv\" \n",
    "subject_dat = pd.read_csv(\"../data/zooni/sud-project-subjects.csv\")\n",
    "df = pd.read_csv(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out test annotations\n",
    "df = df[df['workflow_name'] == 'Q1 - 16 annotators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minh_Trinh</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rhadysh</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SamihaZarin</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sushmitha9</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aryanjain</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chirayupatel</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dwb65</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>geetakukreja</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hassaan.alvi</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>laurenmiller324</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lbouozubaa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>midniteclub</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ms5526</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nsm86</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rr995</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sanjolisogani</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>satvikbhasin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ssm338</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tmk326</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_name  counts\n",
       "0        Minh_Trinh      56\n",
       "1           Rhadysh     301\n",
       "2       SamihaZarin      15\n",
       "3        Sushmitha9     305\n",
       "4         aryanjain     303\n",
       "5      chirayupatel     317\n",
       "6             dwb65       3\n",
       "7      geetakukreja      50\n",
       "8      hassaan.alvi     119\n",
       "9   laurenmiller324     276\n",
       "10       lbouozubaa       1\n",
       "11      midniteclub      69\n",
       "12           ms5526      12\n",
       "13            nsm86     304\n",
       "14            rr995     305\n",
       "15    sanjolisogani     316\n",
       "16     satvikbhasin       1\n",
       "17           ssm338      31\n",
       "18           tmk326      11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of annotations per user\n",
    "df.groupby('user_name').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as per meeting 11/30 - filter annotations made by midniteclub\n",
    "df = df[df['user_name'] != 'midniteclub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse annotation from json\n",
    "df['connection'] = df['annotations'].apply(lambda x: eval(x)[0]['value'])\n",
    "df['subject'] = df['annotations'].apply(lambda x: eval(x)[1]['value'])\n",
    "df['objective'] = df['annotations'].apply(lambda x: eval(x)[2]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"92915036\":{\"retired\":null,\"Filename\":\"text_2483.txt\"}}\n",
    "files = []\n",
    "for line in df[\"subject_data\"]:\n",
    "     # Replace 'null' with 'None' and parse the line as a dictionary\n",
    "    line = line.replace('null', 'None')\n",
    "    line_dict = eval(line)\n",
    "    # Access the first (and only) value in the dictionary\n",
    "    first_value = list(line_dict.values())[0]\n",
    "    # Extract the filename\n",
    "    filename = first_value.get(\"Filename\")\n",
    "    if filename is not None:\n",
    "        # Add the filename to the dataframe as a new column\n",
    "       files.append(filename)\n",
    "\n",
    "df['file'] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get post text\n",
    "subject_dat['file'] = subject_dat['metadata'].apply(lambda x: eval(x)['Filename'])\n",
    "subject_dat['location'] = subject_dat['locations'].apply(lambda x: eval(x)['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter \n",
    "subject_dat = subject_dat[subject_dat['subject_set_id'] == 116963]\n",
    "subject_dat = subject_dat[['subject_id', 'file', 'location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text data from URL\n",
    "# import requests\n",
    "# subject_dat['text'] = subject_dat['location'].apply(lambda x: requests.get(x).text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of posts annotated 1, 2, or 3 times:  counts\n",
      "1    974\n",
      "2    536\n",
      "3    177\n",
      "4     36\n",
      "5      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# how many posts have been annotated more than once\n",
    "annotation_counts = df.groupby('file').size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "print(\"number of posts annotated 1, 2, or 3 times: \", annotation_counts.groupby('counts').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of annotations:  2726\n",
      "num of unique posts:  1724\n",
      "num of posts that received at least 3 annotations:  214\n"
     ]
    }
   ],
   "source": [
    "print(\"num of annotations: \", len(df))\n",
    "print(\"num of unique posts: \", len(df['file'].unique()))\n",
    "print(\"num of posts that received at least 3 annotations: \", len(annotation_counts[annotation_counts['counts'] >= 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**For subset of posts with 2 annotations - re-merge them back in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as per meeting 11/30 - subset the annotations that have only 2 annotations\n",
    "annotation_2counts = annotation_counts[annotation_counts['counts'] == 2]\n",
    "# # unique files\n",
    "annotation_2counts = annotation_2counts[['file']]\n",
    "# #merge with post text\n",
    "annotation_2counts = annotation_2counts.merge(subject_dat, on='file', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation_2counts.to_csv(\"../data/zooni/annotation_2counts_1204.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge back in\n",
    "df_2counts = pd.read_csv(\"../data/zooni/SUD Project Final Round_0104.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files2=[]\n",
    "for line in df_2counts[\"subject_data\"]:\n",
    "    # Replace 'null' with 'None' and parse the line as a dictionary\n",
    "    line = line.replace('null', 'None')\n",
    "    line_dict = eval(line)\n",
    "    # Access the first (and only) value in the dictionary\n",
    "    first_value = list(line_dict.values())[0]\n",
    "    # Extract the filename\n",
    "    filename = first_value.get(\"Filename\")\n",
    "    if filename is not None:\n",
    "        # Add the filename to the dataframe as a new column\n",
    "       files2.append(filename)\n",
    "df_2counts['file'] = files2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of annotations:  302\n",
      "num of unique posts:  244\n"
     ]
    }
   ],
   "source": [
    "print(\"num of annotations: \", len(df_2counts))\n",
    "print(\"num of unique posts: \", len(df_2counts['file'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by username count\n",
    "# df_2counts.groupby('user_name').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Assuming 'annotations' is a string representation of a list\n",
    "df_2counts['annotations'] = df_2counts['annotations'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(index, annotation_list):\n",
    "    if len(annotation_list) > index:\n",
    "        return annotation_list[index]['value']\n",
    "    else:\n",
    "        return None  # Adjust this according to your needs\n",
    "\n",
    "df_2counts['connection'] = df_2counts['annotations'].apply(lambda x: get_value(0, x))\n",
    "df_2counts['subject'] = df_2counts['annotations'].apply(lambda x: get_value(1, x))\n",
    "df_2counts['objective'] = df_2counts['annotations'].apply(lambda x: get_value(2, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2counts = df_2counts[['user_name','subject_ids','file', 'connection', 'subject', 'objective']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subjet data\n",
    "subject_2counts = pd.read_csv(\"../data/zooni/SUD Project Final Round Subjects.csv\")\n",
    "subject_2counts = subject_2counts[subject_2counts['workflow_id'] == 25653]\n",
    "subject_2counts['file'] = subject_2counts['metadata'].apply(lambda x: eval(x)['Filename'])\n",
    "subject_2counts['location'] = subject_2counts['locations'].apply(lambda x: eval(x)['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get location and text\n",
    "import requests\n",
    "\n",
    "subject_2counts['text'] = subject_2counts['location'].apply(lambda x: requests.get(x).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of unique posts:  536\n"
     ]
    }
   ],
   "source": [
    "# are filnenames unique\n",
    "print(\"num of unique posts: \", len(subject_2counts['file'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counts\n",
       "1    195\n",
       "2     41\n",
       "3      7\n",
       "4      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2counts = df_2counts.merge(subject_2counts[['file','text']], on='file', how='left')\n",
    "# how many posts have been annotated more than once\n",
    "df_2counts.groupby('text').size().reset_index(name='counts').sort_values(by='counts', ascending=False).groupby('counts').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(subject_2counts, open(\"../data/zooni/subject_2counts_0104.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add TEXT FOR DF\n",
    "import pickle\n",
    "\n",
    "subject_dat = pickle.load(open(\"../data/zooni/subject_dat.pkl\", \"rb\"))\n",
    "\n",
    "# left join to get text\n",
    "df = df.merge(subject_dat, on='file', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append with df\n",
    "df = df[['user_name','subject_ids','file', 'connection', 'subject', 'objective', 'text']]\n",
    "df_merged = pd.concat([df, df_2counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many with non nan values for connection_y count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df2counts with df on text\n",
    "# df_merge = df.merge(df_2counts[['text','connection','subject','objective']], on='text', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BECAUSE THE SUBJECT IDS AND FILENAMES FOR THE SAME POST TEXT ARE DIFFERENT BETWEEN THE TWO DATASETS, I AM CREATING A NEW IDENTIFIER\n",
    "# ANNOTATIONS OF THE SAME POST TEXT WILL HAVE THE SAME IDENTIFIER FOLLOWED BY THE ANNOTATION NUMBER ex. 1_1 is post 1, annotation 1\n",
    "\n",
    "# group by text and assign new identifier\n",
    "df_merged['id'] = df_merged.groupby('text').ngroup()\n",
    "df_merged['annotation_num'] = df_merged.groupby('id').cumcount()+1\n",
    "df_merged['combo_id'] = df_merged['id'].astype(str) + \"_\" + df_merged['annotation_num'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of posts with at least 3 annotations:  221\n"
     ]
    }
   ],
   "source": [
    "# subset to posts that have been annotated 3 & 4 & 5 times = 823 rows\n",
    "df_merged.groupby('id').size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "print(\"number of posts with at least 3 annotations: \", len(df_merged.groupby('id').size().reset_index(name='counts').sort_values(by='counts', ascending=False)[df_merged.groupby('id').size().reset_index(name='counts').sort_values(by='counts', ascending=False)['counts'] >= 3]))\n",
    "# filter to posts that have been annotated >= 3\n",
    "df_subset = df_merged[df_merged['id'].isin(df_merged.groupby('id').size().reset_index(name='counts').sort_values(by='counts', ascending=False)[df_merged.groupby('id').size().reset_index(name='counts').sort_values(by='counts', ascending=False)['counts'] >= 3]['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of unique posts:  221\n",
      "num of unique annotations:  705\n"
     ]
    }
   ],
   "source": [
    "# get stats for the annotated posts\n",
    "print(\"num of unique posts: \", len(df_subset['id'].unique()))\n",
    "print(\"num of unique annotations: \", len(df_subset['combo_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_subset, open(\"../data/zooni/annotations_0104_complete.pkl\", \"wb\"))\n",
    "# pickle.dump(subject_dat, open(\"../data/zooni/subject_dat.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### label selection\n",
    "\n",
    "select the label to be used for the analysis for posts that have received all three annotations. selection is based on majority - meaning that if two or more annotators have selected the same label, this label will be used for the analysis. if no majority is found, the post will be excluded temporarily until agreement is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df into 3 dfs\n",
    "texts = df_subset[['id', 'text']].drop_duplicates()\n",
    "connection = df_subset[['file', 'connection', 'text', 'id', 'combo_id']]\n",
    "objective = df_subset[['file', 'objective', 'text', 'id', 'combo_id']]\n",
    "subjective = df_subset[['file', 'subject', 'text', 'id', 'combo_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by file and connection\n",
    "connection2 = connection.groupby('id')['connection'].apply(list).reset_index()\n",
    "# for each group, get the most common annotation, keep text\n",
    "connection2['connection'] = connection2['connection'].apply(lambda x: max(set(x), key=x.count)) \n",
    "# add text\n",
    "connection2 = connection2.merge(texts, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(connection2, open(\"../data/zooni/connection.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject2 = subjective.groupby('id')['subject'].apply(list).reset_index()\n",
    "subject2['subject'] = subject2['subject'].apply(lambda x: max(set(x), key=x.count))\n",
    "subject2 = subject2.merge(texts, on='id', how='left')\n",
    "pickle.dump(subject2, open(\"../data/zooni/subject.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num unique posts\n",
    "len(subjective['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_6452/2911549128.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  objective['objective'] = objective['objective'].fillna('')  # Replace None with an empty string\n"
     ]
    }
   ],
   "source": [
    "# transpose and treat each class as a separate column\n",
    "objectives = ['Quality', 'Legality', 'Effects', 'Methods of Ingestion', 'Combination of Substances', 'Mental Health',\n",
    "          'N/A', 'Other', 'Overdose', 'Nurturant Support & Morality', 'Withdrawal', 'Safety', 'Relapse']\n",
    "\n",
    "# create a column for each topic\n",
    "obj = pd.DataFrame(columns=objectives)\n",
    "\n",
    "objective['objective'] = objective['objective'].fillna('')  # Replace None with an empty string\n",
    "\n",
    "for t in objectives:\n",
    "    obj[t] = objective['objective'].apply(lambda x: 1 if t in x else 0)\n",
    "\n",
    "obj = obj.fillna(0)\n",
    "\n",
    "# add the text column\n",
    "obj['Text'] = objective['text']\n",
    "obj['id'] = objective['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by file and get the most common annotation for each class of objective\n",
    "obj2 = obj.groupby('Text').agg(lambda x:x.value_counts().index[0])\n",
    "obj2 = obj2.reset_index()\n",
    "obj2.rename(columns={'Text': 'text'}, inplace=True)\n",
    "# obj2 = obj2.merge(subject_dat, on='text')\n",
    "pickle.dump(obj2, open(\"../data/zooni/objective.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**CREATE TEST SET FOR BOB TEAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 100 posts\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "test_set = random.sample(list(obj2['text']), 100)\n",
    "test_set = pd.DataFrame(test_set, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "test_set.to_csv(\"../data/test_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Caluclate agreement for each label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fleiss kappa\n",
    "\n",
    "# connection2['connection'].value_counts()\n",
    "# subject2['subject'].value_counts()\n",
    "# obj2['Quality'].value_counts()\n",
    "# obj2['Legality'].value_counts()\n",
    "# obj2['Effects'].value_counts()\n",
    "# obj2['Methods of Ingestion'].value_counts()\n",
    "# obj2['Combination of Substances'].value_counts()\n",
    "# obj2['Mental Health'].value_counts()\n",
    "# obj2['N/A'].value_counts()\n",
    "# obj2['Other'].value_counts()\n",
    "# obj2['Overdose'].value_counts()\n",
    "# obj2['Nurturant Support & Morality'].value_counts()\n",
    "# obj2['Withdrawal'].value_counts()\n",
    "# obj2['Safety'].value_counts()\n",
    "# obj2['Relapse'].value_counts()\n",
    "\n",
    "# Fleiss' kappa\n",
    "# from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# fk_connection = cohen_kappa_score(connection['connection'], connection['file'])\n",
    "# fk_subject = cohen_kappa_score(subjective['subject'], subjective['file'])\n",
    "\n",
    "# print(\"Fleiss' kappa for connection: \", fk_connection)\n",
    "# print(\"Fleiss' kappa for subject: \", fk_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff as kd\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "\n",
    "# subset dat\n",
    "dat = connection[['file', 'connection']]\n",
    "# transpose - raters as columns, subjects as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows as dict\n",
    "# blah = dat.head().to_dict()\n",
    "# blah = pd.DataFrame(blah)\n",
    "# transposed_blah = blah.pivot_table(index='file', columns=df.groupby('file').cumcount() + 1, values='connection', aggfunc='first').reset_index()\n",
    "# transposed_blah.columns = ['file', 'rater1', 'rater2', 'rater3']\n",
    "# print(transposed_blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah=connection[connection['id'] == 460]['connection'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for posts that have 4,5 annotations, keep 3 that are most common\n",
    "connection3 = {}\n",
    "for idnum in connection['id']:\n",
    "    subset = connection[connection['id'] == idnum]\n",
    "    if len(subset) > 3:\n",
    "        counts = subset['connection'].value_counts()\n",
    "        # if the freq of one lable is greater than the other two combined, keep that label\n",
    "        if len(counts) > 1 and counts[0] != counts[1]:\n",
    "            label = counts.index[0]\n",
    "            subset = subset[subset['connection'] == label]\n",
    "            subset = subset.head(3)\n",
    "            connection3[idnum] = subset \n",
    "        else:\n",
    "            # if there is a tie, keep the first 3\n",
    "            subset = subset.head(3)\n",
    "            connection3[idnum] = subset    \n",
    "    # else if the number of annotations is 3, keep all 3\n",
    "    else:\n",
    "        connection3[idnum] = subset\n",
    "connection3 = pd.concat(connection3.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get agreement\n",
    "connection4 = connection3.pivot_table(index='id', columns=connection3.groupby('id').cumcount() + 1, values='connection', aggfunc='first').reset_index()\n",
    "connection4.columns = ['file', 'rater1', 'rater2', 'rater3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   file       rater1       rater2       rater3\n",
      "0     7   Disclosure   Disclosure   Disclosure\n",
      "1    24  Inquisition  Inquisition  Inquisition\n",
      "2    26   Disclosure  Inquisition   Disclosure\n",
      "3    37  Inquisition   Disclosure   Disclosure\n",
      "4    38   Disclosure   Disclosure   Disclosure\n"
     ]
    }
   ],
   "source": [
    "print(connection4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to integer codes\n",
    "connection4[['rater1', 'rater2', 'rater3']] = connection4[['rater1', 'rater2', 'rater3']].apply(lambda x: x.astype('category').cat.codes)\n",
    "\n",
    "# export\n",
    "# connection3.to_csv(\"../data/zooni/connection_transpose_agreement.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent agreement:  0.6787330316742082\n"
     ]
    }
   ],
   "source": [
    "# get percent agreement\n",
    "connection4['agreement'] = connection4.apply(lambda x: 1 if x['rater1'] == x['rater2'] == x['rater3'] else 0, axis=1)\n",
    "print(\"Percent agreement: \", connection4['agreement'].sum()/len(connection4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_data=connection4[['rater1', 'rater2', 'rater3']]\n",
    "reliability_data2 = reliability_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha for connection:  0.5625360659704771\n"
     ]
    }
   ],
   "source": [
    "import krippendorff as kd\n",
    "kappa = kd.alpha(reliability_data=reliability_data2, level_of_measurement='nominal')\n",
    "print(\"Krippendorff's alpha for connection: \", kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: subject, dtype: int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah = subjective[subjective['id'] == 44]['subject'].value_counts()\n",
    "blah.sort_values(ascending=False)\n",
    "# keep 3 most common - whatever is in the first row plus the next if the total is not 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject3 = {}\n",
    "for idnum in subjective['id']:\n",
    "    subset = subjective[subjective['id'] == idnum]\n",
    "    if len(subset) > 3:\n",
    "        counts = subset['subject'].value_counts().sort_values(ascending=False)\n",
    "        # 3 same and 1 or 2 different\n",
    "        if counts[0] == 3 :\n",
    "            label = counts.index[0]\n",
    "            subset = subset[subset['subject'] == label]\n",
    "            subject3[idnum] = subset \n",
    "        # 2 x 2 tie\n",
    "        elif counts[0] == 2:\n",
    "            subset = subset.head(3)\n",
    "            subject3[idnum] = subset\n",
    "        # if all 4 or 5 labels are the same, or none of the labels are the same\n",
    "        else:\n",
    "            subset = subset.head(3)\n",
    "            subject3[idnum] = subset    \n",
    "    # else if the number of annotations is 3, keep all 3\n",
    "    else:\n",
    "        subject3[idnum] = subset\n",
    "subject3 = pd.concat(subject3.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject4 = subject3.pivot_table(index='id', columns=subject3.groupby('id').cumcount() + 1, values='subject', aggfunc='first').reset_index()\n",
    "subject4.columns = ['id', 'rater1', 'rater2', 'rater3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluations, export subject4 with text\n",
    "subject5 = subject4.merge(texts, on='id', how='left')\n",
    "# filter for just 100% disagreements\n",
    "subject5 = subject5[subject5['rater1'] != subject5['rater2']]\n",
    "subject5.to_csv(\"../data/zooni/subject_transpose_agreement.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to categorical\n",
    "subject4[['rater1', 'rater2', 'rater3']] = subject4[['rater1', 'rater2', 'rater3']].apply(lambda x: x.astype('category').cat.codes)\n",
    "\n",
    "# export\n",
    "# subject3.to_csv(\"../data/zooni/subject_transpose_agreement.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent agreement:  0.29411764705882354\n"
     ]
    }
   ],
   "source": [
    "subject4['agreement'] = subject4.apply(lambda x: 1 if x['rater1'] == x['rater2'] == x['rater3'] else 0, axis=1)\n",
    "print(\"Percent agreement: \", subject4['agreement'].sum()/len(subject4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha for subject:  0.2113871693472884\n"
     ]
    }
   ],
   "source": [
    "reliability_data=subject4[['rater1', 'rater2', 'rater3']]\n",
    "reliability_data2 = reliability_data.T\n",
    "\n",
    "kappa = kd.alpha(reliability_data=reliability_data2, level_of_measurement='nominal')\n",
    "print(\"Krippendorff's alpha for subject: \", kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective\n",
    "# AS OF 12/11, WE DECIDED NOT TO DO/REPORT AGREEMENT FOR OBJECTIVE\n",
    "# objective3 = {}\n",
    "# for file in obj['file']:\n",
    "#     if len(obj[obj['file'] == file]) > 3:\n",
    "#         objective3[file] = obj[objective['file'] == file].iloc[0:3]\n",
    "#     else:\n",
    "#         objective3[file] = obj[objective['file'] == file]\n",
    "\n",
    "# objective3 = pd.concat(objective3.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to 3d array\n",
    "# arrays = []\n",
    "# for objective in objectives:\n",
    "#     arrays.append(objective3.pivot_table(index='file', columns=df.groupby('file').cumcount() + 1, values=objective, aggfunc='first').reset_index())\n",
    "#     # rename\n",
    "#     arrays[-1].columns = ['file', 'rater1', 'rater2', 'rater3']\n",
    "#     # add agreement column\n",
    "#     arrays[-1]['agreement'] = arrays[-1].apply(lambda x: 1 if x['rater1'] == x['rater2'] == x['rater3'] else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percent agreement for each objective\n",
    "# for i in range(len(arrays)):\n",
    "#     print(\"Percent agreement for \", objectives[i], \": \", arrays[i]['agreement'].sum()/len(arrays[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate kappa for each objective\n",
    "# for i in range(len(arrays)):\n",
    "#     reliability_data=arrays[i][['rater1', 'rater2', 'rater3']]\n",
    "#     reliability_data2 = reliability_data.T\n",
    "#     kappa = kd.alpha(reliability_data=reliability_data2, level_of_measurement='nominal')\n",
    "#     print(\"Krippendorff's alpha for \", objectives[i], \": \", kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Annotation Inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distribution of annotation per student to assess if they are injecting noise\n",
    "# how are the student annotating? are they annotating everything as 'other'?\n",
    "df.groupby('user_name').size().reset_index(name='counts')\n",
    "# get distribution of annotations per student\n",
    "df.groupby('user_name')['connection'].value_counts().reset_index(name='counts')\n",
    "# get percentages\n",
    "# df.groupby('user_name')['connection'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('user_name')['subject'].value_counts().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of subject annotations per student\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"user_name\", y=\"counts\", hue=\"subject\", data=df.groupby('user_name')['subject'].value_counts().reset_index(name='counts'))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the proportion of each class of subject annotation per student?\n",
    "# as a percentage of all annotations\n",
    "subject_by_student = df.groupby('user_name')['subject'].value_counts(normalize=True).reset_index(name='proportion')\n",
    "subject_by_student['counts'] = df.groupby('user_name')['subject'].value_counts().reset_index(name='counts')['counts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
