{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "# data\n",
    "dat = pickle.load(open('../data/zooni/annotations_1105.pkl', 'rb'))\n",
    "\n",
    "import torch\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_58058/3564290673.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subj['subject'] = le.fit_transform(subj['subject'])\n"
     ]
    }
   ],
   "source": [
    "# create connection df\n",
    "connection = dat[['connection', 'text']]\n",
    "\n",
    "subj = dat[['subject', 'text']]\n",
    "\n",
    "# encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "subj['subject'] = le.fit_transform(subj['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TYPE OF CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# make training and evaluation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, eval = train_test_split(connection, test_size=0.2, random_state=42, stratify=connection['connection'])\n",
    "\n",
    "# Extract text and labels from your DataFrame\n",
    "x = list(train['text'].values)\n",
    "# convert string labels to numeric values, 1 for inquisition and 2 for disclosure\n",
    "labels = [1 if label == 'Inquisition' else 0 for label in train['connection'].values]\n",
    "\n",
    "\n",
    "## Initialize the tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Encode the text data and convert labels to tensors\n",
    "inputs = tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to('mps')\n",
    "labels = torch.tensor(labels).to('mps')\n",
    "\n",
    "# create dataset\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# create training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "inputs.to(device)\n",
    "labels.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlab218\u001b[0m (\u001b[33msocial-nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/wandb/run-20231105_170226-s8ft0qqt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/social-nlp/huggingface/runs/s8ft0qqt' target=\"_blank\">clear-waterfall-5</a></strong> to <a href='https://wandb.ai/social-nlp/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/social-nlp/huggingface' target=\"_blank\">https://wandb.ai/social-nlp/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/social-nlp/huggingface/runs/s8ft0qqt' target=\"_blank\">https://wandb.ai/social-nlp/huggingface/runs/s8ft0qqt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 10/80 [00:14<00:54,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0005, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 10/80 [00:18<00:54,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1460366249084473, 'eval_runtime': 3.8745, 'eval_samples_per_second': 16.776, 'eval_steps_per_second': 1.29, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [00:28<00:45,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1491, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 20/80 [00:29<00:45,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.06001877784729, 'eval_runtime': 0.8273, 'eval_samples_per_second': 78.573, 'eval_steps_per_second': 6.044, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 30/80 [00:38<00:37,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6043, 'learning_rate': 3e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 38%|███▊      | 30/80 [00:39<00:37,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9366815090179443, 'eval_runtime': 0.8511, 'eval_samples_per_second': 76.374, 'eval_steps_per_second': 5.875, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [00:49<00:29,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4544, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 50%|█████     | 40/80 [00:49<00:29,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7842634916305542, 'eval_runtime': 0.8554, 'eval_samples_per_second': 75.988, 'eval_steps_per_second': 5.845, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 50/80 [00:59<00:21,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2072, 'learning_rate': 5e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 62%|██████▎   | 50/80 [01:00<00:21,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.585748314857483, 'eval_runtime': 0.8561, 'eval_samples_per_second': 75.928, 'eval_steps_per_second': 5.841, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [01:09<00:14,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1047, 'learning_rate': 6e-06, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 75%|███████▌  | 60/80 [01:10<00:14,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2772217988967896, 'eval_runtime': 0.8543, 'eval_samples_per_second': 76.086, 'eval_steps_per_second': 5.853, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 70/80 [01:20<00:07,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8822, 'learning_rate': 7.000000000000001e-06, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 88%|████████▊ | 70/80 [01:20<00:07,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9954636693000793, 'eval_runtime': 0.84, 'eval_samples_per_second': 77.377, 'eval_steps_per_second': 5.952, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:30<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7727, 'learning_rate': 8.000000000000001e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 80/80 [01:31<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8513472080230713, 'eval_runtime': 0.8669, 'eval_samples_per_second': 74.983, 'eval_steps_per_second': 5.768, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:34<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 105.0896, 'train_samples_per_second': 12.18, 'train_steps_per_second': 0.761, 'train_loss': 1.3968984365463257, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8513472080230713, 'eval_runtime': 1.9465, 'eval_samples_per_second': 33.393, 'eval_steps_per_second': 2.569, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "# Initialize Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming you have a list of text and labels\n",
    "data = {\n",
    "    'text': x,\n",
    "    'labels': labels\n",
    "}\n",
    "evaluation_data = {\n",
    "    'text': list(eval['text'].values),\n",
    "    'labels': [1 if label == 'Inquisition' else 0 for label in eval['connection'].values]\n",
    "}\n",
    "\n",
    "# # Create a dataset\n",
    "# training_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# # Initialize the Trainer with the dataset\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=DataCollatorWithPadding(tokenizer),\n",
    "#     train_dataset=training_dataset,\n",
    "# )\n",
    "\n",
    "# # Start training\n",
    "# trainer.train()\n",
    "\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "eval_dataset = Dataset.from_dict(evaluation_data)\n",
    "# Define a function to preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Preprocess the dataset\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate(encoded_eval_dataset)\n",
    "# Save the model\n",
    "trainer.save_model()\n",
    "\n",
    "# Save the tokenizer\n",
    "# tokenizer.save_pretrained('./results')\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46\n",
      "Precision: 0.49\n",
      "Recall: 0.85\n",
      "F1-Score: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming that 'true_labels' are the ground truth labels and 'predicted_labels' are the model's predicted labels\n",
    "true_labels = [1 if label == 'Inquisition' else 0 for label in eval['connection'].values]\n",
    "predicted_labels = trainer.predict(encoded_eval_dataset).predictions.argmax(-1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-Score: {:.2f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SUBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for subject\n",
    "# make training and evaluation sets\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train, eval = train_test_split(subj, test_size=0.2, random_state=42, stratify=subj['subject'])\n",
    "\n",
    "# Extract text and labels from your DataFrame\n",
    "x = list(train['text'].values)\n",
    "labels = train['subject'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Encode the text data and convert labels to tensors\n",
    "inputs = tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to('mps')\n",
    "labels = torch.tensor(labels).to('mps')\n",
    "\n",
    "# create dataset\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "inputs.to(device)\n",
    "labels.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 10/80 [00:10<00:51,  1.36it/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3131, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 12%|█▎        | 10/80 [00:11<00:51,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.626993656158447, 'eval_runtime': 1.1786, 'eval_samples_per_second': 55.15, 'eval_steps_per_second': 4.242, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [00:21<00:48,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.27, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 25%|██▌       | 20/80 [00:22<00:48,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.429929256439209, 'eval_runtime': 0.9258, 'eval_samples_per_second': 70.211, 'eval_steps_per_second': 5.401, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 30/80 [00:32<00:39,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6979, 'learning_rate': 3e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 38%|███▊      | 30/80 [00:33<00:39,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.067989349365234, 'eval_runtime': 0.8466, 'eval_samples_per_second': 76.774, 'eval_steps_per_second': 5.906, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [00:43<00:29,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4894, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 50%|█████     | 40/80 [00:44<00:29,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.541857719421387, 'eval_runtime': 0.8548, 'eval_samples_per_second': 76.04, 'eval_steps_per_second': 5.849, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 50/80 [00:54<00:22,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9561, 'learning_rate': 5e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 62%|██████▎   | 50/80 [00:54<00:22,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.639289379119873, 'eval_runtime': 0.8523, 'eval_samples_per_second': 76.263, 'eval_steps_per_second': 5.866, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [01:04<00:14,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1089, 'learning_rate': 6e-06, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 75%|███████▌  | 60/80 [01:05<00:14,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8815629482269287, 'eval_runtime': 0.8558, 'eval_samples_per_second': 75.949, 'eval_steps_per_second': 5.842, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 70/80 [01:15<00:07,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9668, 'learning_rate': 7.000000000000001e-06, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 88%|████████▊ | 70/80 [01:15<00:07,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0633935928344727, 'eval_runtime': 0.8284, 'eval_samples_per_second': 78.464, 'eval_steps_per_second': 6.036, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:25<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6237, 'learning_rate': 8.000000000000001e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 80/80 [01:26<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6272696256637573, 'eval_runtime': 0.8734, 'eval_samples_per_second': 74.418, 'eval_steps_per_second': 5.724, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:29<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 89.4733, 'train_samples_per_second': 14.306, 'train_steps_per_second': 0.894, 'train_loss': 4.303243088722229, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6272696256637573, 'eval_runtime': 0.8841, 'eval_samples_per_second': 73.517, 'eval_steps_per_second': 5.655, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "data = {\n",
    "    'text': x,\n",
    "    'labels': labels\n",
    "}\n",
    "evaluation_data = {\n",
    "    'text': list(eval['text'].values),\n",
    "    'labels': eval['subject'].values\n",
    "}\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "eval_dataset = Dataset.from_dict(evaluation_data)\n",
    "# Define a function to preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Preprocess the dataset\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate(encoded_eval_dataset)\n",
    "# Save the model\n",
    "trainer.save_model(\"subject_model\")\n",
    "\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38\n",
      "Precision: 0.38\n",
      "Recall: 0.38\n",
      "F1-Score: 0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get metrics\n",
    "true_labels = eval['subject'].values\n",
    "predicted_labels = trainer.predict(encoded_eval_dataset).predictions.argmax(-1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-Score: {:.2f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## OBJECTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "objectives = ['Quality', 'Legality', 'Effects', 'Methods', 'Combination of Substances', 'Mental Health',\n",
    "          'N/A', 'Other', 'Overdose', 'Nurturant Support & Morality', 'Withdrawal', 'Safety', 'Relapse']\n",
    "\n",
    "# create a column for each topic\n",
    "obj = pd.DataFrame(columns=objectives)\n",
    "\n",
    "for t in objectives:\n",
    "    obj[t] = dat['objective'].apply(lambda x: 1 if t in x else 0)\n",
    "\n",
    "obj = obj.fillna(0)\n",
    "\n",
    "# add the text column\n",
    "obj['text'] = dat['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for objective\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# make training and evaluation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Split your data into training and evaluation sets\n",
    "train_data, eval_data = train_test_split(obj, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GPT-2 tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_inputs = tokenizer(list(train_data['text']), padding=True, truncation=True, return_tensors='pt')\n",
    "eval_inputs = tokenizer(list(eval_data['text']), padding=True, truncation=True, return_tensors='pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to tensors\n",
    "train_labels = torch.tensor(train_data[objectives].values, dtype=torch.long)\n",
    "eval_labels = torch.tensor(eval_data[objectives].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to mps\n",
    "device = \"mps\"\n",
    "train_inputs = train_inputs.to(device)\n",
    "eval_inputs = eval_inputs.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "eval_labels = eval_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 13.40 GB, other allocations: 4.30 GB, max allowed: 18.13 GB). Tried to allocate 468.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb Cell 24\u001b[0m line \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m p: accuracy_score(p\u001b[39m.\u001b[39mpredictions\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), p\u001b[39m.\u001b[39mlabel_ids),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#Y101sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m eval_metrics \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2776\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2779\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:2801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2802\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1419\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1419\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1420\u001b[0m     input_ids,\n\u001b[1;32m   1421\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1422\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1423\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1424\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1425\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1426\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1427\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1428\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1429\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1430\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1431\u001b[0m )\n\u001b[1;32m   1432\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1433\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    901\u001b[0m         hidden_states,\n\u001b[1;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    391\u001b[0m     hidden_states,\n\u001b[1;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:312\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_attn(hidden_states)\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    314\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    315\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/pytorch_utils.py:106\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    105\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 106\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    107\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 13.40 GB, other allocations: 4.30 GB, max allowed: 18.13 GB). Tried to allocate 468.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "#  Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 \n",
    "#  to disable the cache and use the default PyTorch allocator.\n",
    "#  Use PYTORCH_MPS=1 to enable MPS.\n",
    "from transformers import Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create a DataCollator with padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# trainer expects a Dataset object\n",
    "train_dataset = {\n",
    "    'input_ids': train_inputs['input_ids'],\n",
    "    'attention_mask': train_inputs['attention_mask'],\n",
    "    'labels': train_labels\n",
    "}\n",
    "\n",
    "eval_dataset = {\n",
    "    'input_ids': eval_inputs['input_ids'],\n",
    "    'attention_mask': eval_inputs['attention_mask'],\n",
    "    'labels': eval_labels\n",
    "}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "eval_dataset = Dataset.from_dict(eval_dataset)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: accuracy_score(p.predictions.argmax(-1), p.label_ids),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text and labels for training and evaluation\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, text, labels, tokenizer, max_length=128):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.text[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and evaluation datasets\n",
    "train_dataset = MultiLabelDataset(train_data['text'], train_labels, tokenizer)\n",
    "eval_dataset = MultiLabelDataset(eval_data['text'], eval_labels, tokenizer)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiLabelDataset' object has no attribute '_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb Cell 22\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1605\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1603\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrently training with a batch size of: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1604\u001b[0m \u001b[39m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1605\u001b[0m train_dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_train_dataloader()\n\u001b[1;32m   1607\u001b[0m \u001b[39m# Setting up training control variables:\u001b[39;00m\n\u001b[1;32m   1608\u001b[0m \u001b[39m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m \u001b[39m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m \u001b[39m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m total_train_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mworld_size\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:836\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m data_collator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_collator\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m is_datasets_available() \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(train_dataset, datasets\u001b[39m.\u001b[39mDataset):\n\u001b[0;32m--> 836\u001b[0m     train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_remove_unused_columns(train_dataset, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    837\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    838\u001b[0m     data_collator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_collator_with_removed_columns(data_collator, description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:758\u001b[0m, in \u001b[0;36mTrainer._remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_signature_columns_if_needed()\n\u001b[1;32m    756\u001b[0m signature_columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signature_columns\n\u001b[0;32m--> 758\u001b[0m ignored_columns \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(dataset\u001b[39m.\u001b[39;49mcolumn_names) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(signature_columns))\n\u001b[1;32m    759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ignored_columns) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    760\u001b[0m     dset_description \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m description \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the \u001b[39m\u001b[39m{\u001b[39;00mdescription\u001b[39m}\u001b[39;00m\u001b[39m set\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:1727\u001b[0m, in \u001b[0;36mDataset.column_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   1715\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolumn_names\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1716\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Names of the columns in the dataset.\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m   1726\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1727\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39mcolumn_names\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiLabelDataset' object has no attribute '_data'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"objective_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "train_inputs.to(device)\n",
    "eval_inputs.to(device)\n",
    "train_labels.to(device)\n",
    "eval_labels.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': list(train_data['text'].values),\n",
    "    'labels': obj\n",
    "}\n",
    "\n",
    "evaluation_data = {\n",
    "    'text': list(eval_data['text'].values),\n",
    "    'labels': eval_data[objectives].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 1 named labels expected length 256 but got length 14",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb Cell 22\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create a Hugging Face dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_dict(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X60sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m eval_dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfrom_dict(evaluation_data)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:882\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    880\u001b[0m     arrow_typed_mapping[col] \u001b[39m=\u001b[39m data\n\u001b[1;32m    881\u001b[0m mapping \u001b[39m=\u001b[39m arrow_typed_mapping\n\u001b[0;32m--> 882\u001b[0m pa_table \u001b[39m=\u001b[39m InMemoryTable\u001b[39m.\u001b[39;49mfrom_pydict(mapping\u001b[39m=\u001b[39;49mmapping)\n\u001b[1;32m    883\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     info \u001b[39m=\u001b[39m DatasetInfo()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:785\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    770\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pydict\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    771\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39m    Construct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[39m        `datasets.table.Table`\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 785\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pydict(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyarrow/table.pxi:3725\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyarrow/table.pxi:5255\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyarrow/table.pxi:3674\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyarrow/table.pxi:2837\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 1 named labels expected length 256 but got length 14"
     ]
    }
   ],
   "source": [
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "eval_dataset = Dataset.from_dict(evaluation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb Cell 20\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create training and evaluation datasets\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m TensorDataset(train_inputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], train_inputs[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], train_labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m eval_dataset \u001b[39m=\u001b[39m TensorDataset(eval_inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], eval_inputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m], eval_labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Define training arguments\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "# Create training and evaluation datasets\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "eval_dataset = TensorDataset(eval_inputs['input_ids'], eval_inputs['attention_mask'], eval_labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/laylabouzoubaa/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:878: UserWarning: unknown class(es) [' ', '&', '/', 'A', 'C', 'E', 'H', 'L', 'M', 'N', 'O', 'R', 'S', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb Cell 18\u001b[0m line \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_labels \u001b[39m=\u001b[39m mlb\u001b[39m.\u001b[39mtransform(train\u001b[39m.\u001b[39miloc[:, \u001b[39m1\u001b[39m:])  \u001b[39m# Assuming label columns start from the second column\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_labels, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m TensorDataset(train_inputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], train_inputs[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], train_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Encode the evaluation data and create the evaluation dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m eval_inputs \u001b[39m=\u001b[39m tokenizer(\u001b[39mlist\u001b[39m(eval_data[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]), padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=len(objectives))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Encode the training data and create the training dataset\n",
    "train_inputs = tokenizer(list(train['text']), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "train_labels = mlb.transform(train.iloc[:, 1:])  # Assuming label columns start from the second column\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "\n",
    "# Encode the evaluation data and create the evaluation dataset\n",
    "eval_inputs = tokenizer(list(eval_data['text']), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "eval_labels = mlb.transform(eval_data.iloc[:, 1:])  # Assuming label columns start from the second column\n",
    "eval_labels = torch.tensor(eval_labels, dtype=torch.float32)\n",
    "eval_dataset = TensorDataset(eval_inputs['input_ids'], eval_inputs['attention_mask'], eval_labels)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "inputs.to(device)\n",
    "labels.to(device)\n",
    "eval_inputs.to(device)\n",
    "eval_labels.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,  # Training batch size\n",
    "    per_device_eval_batch_size=16,   # Evaluation batch size\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laylabouzoubaa/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/70 [10:33<?, ?it/s]\n",
      "  0%|          | 0/140 [08:01<?, ?it/s]\n",
      "  0%|          | 0/140 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb Cell 22\u001b[0m line \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set up and run the Trainer for training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39meval_dataset\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate(eval_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Save the model\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1870\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1867\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1870\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1871\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1872\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[1;32m    452\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer_utils.py:737\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[\u001b[39mdict\u001b[39m]):\n\u001b[1;32m    736\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remove_columns(feature) \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features]\n\u001b[0;32m--> 737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_collator(features)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 249\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    250\u001b[0m         features,\n\u001b[1;32m    251\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    252\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    253\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    254\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    257\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3208\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   3205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[1;32m   3206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3207\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 3208\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3209\u001b[0m     )\n\u001b[1;32m   3211\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   3213\u001b[0m \u001b[39mif\u001b[39;00m required_input \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(required_input, Sized) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(required_input) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(dataloader) * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "# Set up and run the Trainer for training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate(eval_dataset)\n",
    "# Save the model\n",
    "trainer.save_model(\"objective_model\")\n",
    "\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train\n",
    "data = {\n",
    "    'text': x,\n",
    "    'labels': labels\n",
    "}\n",
    "evaluation_data = {\n",
    "    'text': list(eval['text'].values),\n",
    "    'labels': eval[objectives].values\n",
    "}\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "eval_dataset = Dataset.from_dict(evaluation_data)\n",
    "# Define a function to preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Preprocess the dataset\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8) to match target batch_size (16).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb Cell 21\u001b[0m line \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Initialize Trainer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mencoded_eval_dataset\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate(encoded_eval_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drug_personal_exp/py/make_fewshot.ipynb#X44sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Save the model\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2776\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2779\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:2801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2802\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1477\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msingle_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1476\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1477\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(pooled_logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_labels), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1478\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1479\u001b[0m     loss_fct \u001b[39m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (16)."
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset= encoded_dataset,\n",
    "    eval_dataset=encoded_eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate(encoded_eval_dataset)\n",
    "# Save the model\n",
    "trainer.save_model(\"objective_model\")\n",
    "\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics\n",
    "true_labels = eval[].values\n",
    "predicted_labels = trainer.predict(encoded_eval_dataset).predictions.argmax(-1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-Score: {:.2f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"gpt2\"\n",
    "topics = ['Symptoms', 'Guidance', 'Withdrawal', 'Recovery', 'Relapse', 'Sobriety',\n",
    "          'Overdose', 'Social Support', 'Cultural Reference', 'Values', 'Banter', \n",
    "          'Not Applicable']\n",
    "## Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=len(topics))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Encode the text data and convert labels to tensors\n",
    "inputs = tokenizer(list(dat2['Text']), padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to('mps')\n",
    "labels = torch.tensor(dat2[topics].values, dtype=torch.float32).to('mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "inputs.to(device)\n",
    "labels.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training dataset and an evaluation dataset\n",
    "# You can choose the percentage for the evaluation dataset, for example, 10%.\n",
    "eval_split = int(0.1 * len(inputs))\n",
    "eval_inputs, eval_labels = inputs[:eval_split], labels[:eval_split]\n",
    "train_inputs, train_labels = inputs[eval_split:], labels[eval_split:]\n",
    "\n",
    "# Create DataLoaders for both training and evaluation datasets\n",
    "train_dataloader = DataLoader(TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels), batch_size=8)\n",
    "eval_dataloader = DataLoader(TensorDataset(eval_inputs['input_ids'], eval_inputs['attention_mask'], eval_labels), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "# dataset = TensorDataset(inputs.input_ids, inputs.attention_mask, labels)\n",
    "# dataloader = DataLoader(dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-fine-tuned\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laylabouzoubaa/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "# Initialize the optimizer (AdamW)\n",
    "# Initialize the learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_training_steps = len(train_dataloader) * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Trainer with optimizer and scheduler\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=train_dataloader,\n",
    "#     eval_dataset=eval_dataloader,  # Include the evaluation dataset\n",
    "#     optimizers=(optimizer, scheduler),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Training Loss: 0.26106315513826767\n",
      "Epoch 2, Average Training Loss: 0.25861914118505874\n",
      "Epoch 3, Average Training Loss: 0.2594894623419024\n",
      "Epoch 4, Average Training Loss: 0.26018973144720187\n",
      "Epoch 5, Average Training Loss: 0.2602604152459019\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation predictions is empty, cannot perform evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Manually perform the training loop\n",
    "# Lists to store training loss for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "training_losses = []\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        batch_mps = {\n",
    "            'input_ids': batch[0].to('mps'),\n",
    "            'attention_mask': batch[1].to('mps'),\n",
    "            'labels': batch[2].to('mps')\n",
    "        }\n",
    "        # input_ids, attention_mask, labels = batch\n",
    "        # input_ids = input_ids.to(device)\n",
    "        # attention_mask = attention_mask.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch_mps)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    training_losses.append(avg_train_loss)  # Append training loss for visualization\n",
    "    print(f\"Epoch {epoch + 1}, Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "# Plot and visualize the training loss\n",
    "plt.plot(training_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.show()\n",
    "\n",
    "# Manually predict and evaluate the model\n",
    "eval_predictions = []\n",
    "eval_labels = []\n",
    "# Check if the evaluation dataset is not empty\n",
    "if eval_inputs and 'input_ids' in eval_inputs:\n",
    "    for batch in eval_dataloader:\n",
    "        batch_mps = {\n",
    "            'input_ids': batch['input_ids'].to('mps'),\n",
    "            'attention_mask': batch['attention_mask'].to('mps'),\n",
    "            'labels': batch['labels']\n",
    "        }\n",
    "        print('input_ids:', batch_mps['input_ids'])\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=batch_mps['input_ids'], attention_mask=batch_mps['attention_mask'])\n",
    "            eval_predictions.append(output.logits)\n",
    "            eval_labels.append(batch_mps['labels'])\n",
    "            print('logits:', output.logits)\n",
    "            print('labels:', batch_mps['labels'])\n",
    "\n",
    "\n",
    "    if len(eval_predictions) > 0:\n",
    "        eval_predictions = torch.cat(eval_predictions, dim=0)\n",
    "        eval_labels = torch.cat(eval_labels, dim=0)\n",
    "        eval_loss = torch.nn.BCEWithLogitsLoss()(eval_predictions, eval_labels).mean()\n",
    "\n",
    "        # Convert predictions and labels to NumPy arrays\n",
    "        eval_predictions = eval_predictions.cpu().numpy()\n",
    "        eval_labels = eval_labels.cpu().numpy()\n",
    "\n",
    "        # Calculate accuracy, precision, recall, and F1 score\n",
    "        accuracy = accuracy_score(eval_labels, (eval_predictions > 0.5).astype(int))\n",
    "        precision = precision_score(eval_labels, (eval_predictions > 0.5).astype(int), average='micro')\n",
    "        recall = recall_score(eval_labels, (eval_predictions > 0.5).astype(int), average='micro')\n",
    "        f1 = f1_score(eval_labels, (eval_predictions > 0.5).astype(int), average='micro')\n",
    "\n",
    "        # Print evaluation metrics\n",
    "        print(\"***** Evaluation Metrics *****\")\n",
    "        print(f\"Eval Loss: {eval_loss.item()}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "    else:\n",
    "        print(\"Evaluation predictions is empty, cannot perform evaluation.\")\n",
    "else:\n",
    "    print(\"Evaluation dataset is empty, cannot perform evaluation.\")\n",
    "    # pass # Skip the evaluation step if the evaluation dataset is empty\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./gpt2-fine-tuned2\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
